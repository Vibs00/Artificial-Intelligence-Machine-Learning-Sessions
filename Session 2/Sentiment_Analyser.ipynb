{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentiment Analyser.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "hv-1rpgimMLe"
      },
      "source": [
        "!pip install kaggle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wBQVTjpfyang"
      },
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-RT5c0slyk8M"
      },
      "source": [
        "#Make a directory named kaggle and copy the kaggle.json file there.\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "# change the permission of the file\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F8rlah1RzQee"
      },
      "source": [
        "!kaggle datasets download -d snap/amazon-fine-food-reviews"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xc7wGQ26z_Ga",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65bc6215-1d36-42af-dfeb-3201484e6a70"
      },
      "source": [
        "#unzipping zip\n",
        "from zipfile import ZipFile\n",
        "file_name = 'amazon-fine-food-reviews.zip' #the file is your dataset exact name\n",
        "with ZipFile(file_name, 'r') as zips:\n",
        "  zips.extractall()\n",
        "  print('Done')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3YmOjPfp3P8v"
      },
      "source": [
        "#libraries used in the model: pandas\n",
        "#reading csv file\n",
        "import pandas as pd\n",
        "\n",
        "def read_file_chunks(file_name):\n",
        "  return pd.read_csv(file_name, chunksize =10000)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39Xcnzx2QbX7"
      },
      "source": [
        "#reading first chunk\n",
        "df =read_file_chunks('Reviews.csv')"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qH11D2o_QvF2"
      },
      "source": [
        "#checking our DataFrame\n",
        "next(df).head()\n",
        "# df.shape\n",
        "# df.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aklvf_J1Q-64"
      },
      "source": [
        "#self analysis : can ignore this cell\n",
        "# horrible, \n",
        "df =read_file_chunks('Reviews.csv')\n",
        "\n",
        "pd.set_option(\"display.max_colwidth\", 2)\n",
        "i=0\n",
        "for data in df:\n",
        "  i +=10000\n",
        "  # good =data['Summary'].str.find(\"wonderful\")\n",
        "  # good_bad =data[good >0]['Score'] ==1\n",
        "  # print(data.loc[good_bad[good_bad ==True].index]['Summary'])\n",
        "  # if i ==250000:\n",
        "  #   print(data.loc[244580])\n",
        "  print(data[data['Summary'] =='These \"Butter Leaves\" are addictively wonderful!'][['Score', 'Text']])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HzifpQIXTOY5"
      },
      "source": [
        "#visualizing the data\n",
        "import matplotlib.pyplot as plt\n",
        "df =read_file_chunks('Reviews.csv')\n",
        "\n",
        "dic ={'1':0, '2':0, '3':0, '4':0, '5':0}\n",
        "\n",
        "def sore_count(obj):\n",
        "  dic['1'] +=obj[obj ==1].count()\n",
        "  dic['2'] +=obj[obj ==2].count()\n",
        "  dic['3'] +=obj[obj ==3].count()\n",
        "  dic['4'] +=obj[obj ==4].count()\n",
        "  dic['5'] +=obj[obj ==5].count()\n",
        "\n",
        "for chunk in df:\n",
        "  chunk[['Score']].apply(sore_count)\n",
        "\n",
        "plt.bar(dic.keys(), dic.values())\n",
        "plt.show()\n",
        "plt.clf()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iaGC1TH7raae"
      },
      "source": [
        "#make a wordcloud for entire data\n",
        "df =read_file_chunks('Reviews.csv')\n",
        "\n",
        "#removing stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from wordcloud import WordCloud\n",
        "STOPWORDS =stopwords.words()\n",
        "# print(len(stopwords.words('english')))\n",
        "\n",
        "#find more stopwords\n",
        "# for chunk in df:\n",
        "#   print(chunk[chunk['Text'].str.find('<a') >0])\n",
        "\n",
        "#stopwords to add : <br />, <p>, </p>, <a, href\n",
        "#after run add stopwords : 'use', 'got', 'th', 'ordered', 'bought'\n",
        "STOPWORDS.extend(['br', '<p>', '</p>', '<a', 'href', 'use', 'got', 'th', 'ordered', 'bought'])\n",
        "# print(STOPWORDS)\n",
        "\n",
        "#extracting non stopwords from 'Text' column\n",
        "text = \" \".join(chunk.Text.to_string() for chunk in df)\n",
        "wordcloud =WordCloud(stopwords=STOPWORDS).generate(text)\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L27WClCL0I4i"
      },
      "source": [
        "import numpy as np\n",
        "df =read_file_chunks('Reviews.csv')\n",
        "#we use header only once i.e. at the start i.e. on the first chunk\n",
        "#if we forget this header will be appended at every chunk.\n",
        "#Therefore increasing number of records\n",
        "\n",
        "header =True\n",
        "for chunk in df:\n",
        "  chunk['Sentiment'] = np.where(chunk['Score']<=2, -1, chunk['Score'])\n",
        "  chunk['Sentiment'] = np.where(chunk['Sentiment']==3, 0, chunk['Sentiment'])\n",
        "  chunk['Sentiment'] = np.where(chunk['Sentiment']>=4, 1, chunk['Sentiment'])\n",
        "  #first delete then append\n",
        "  chunk.to_csv(\"new_file_Reviews.csv\",index =False, header =header, mode='a')\n",
        "  header =False\n",
        "\n",
        "df =read_file_chunks('new_file_Reviews.csv')\n",
        "next(df).head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQYZ2Yw_09bU"
      },
      "source": [
        "#good reviews\n",
        "df =read_file_chunks('new_file_Reviews.csv')\n",
        "text = \" \".join(chunk[chunk['Sentiment'] ==1].Summary.to_string() for chunk in df)\n",
        "wordcloud =WordCloud(stopwords=STOPWORDS).generate(text)\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "INIA7-0G8CZ_"
      },
      "source": [
        "#neutral reviews\n",
        "df =read_file_chunks('new_file_Reviews.csv')\n",
        "text = \" \".join(chunk[chunk['Sentiment'] ==0].Summary.to_string() for chunk in df)\n",
        "wordcloud =WordCloud(stopwords=STOPWORDS).generate(text)\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R0jUzTVjmcZ8"
      },
      "source": [
        "#visualizing data wrt words\n",
        "import seaborn as sns\n",
        "words ={'good': [0]*5, 'great': [0]*5, 'like': [0]*5, 'love': [0]*5, 'bad': [0]*5, 'horrible': [0]*5}\n",
        "i =0\n",
        "\n",
        "def word_search(obj):\n",
        "  #getting obj from score =1 to score =5 i.e. asc order\n",
        "  global words, i\n",
        "  for key, values in words.items():\n",
        "    words[key][i] +=obj[obj.str.lower().str.find(key) >=0].count()\n",
        "  i +=1\n",
        "\n",
        "df =read_file_chunks('Reviews.csv')\n",
        "\n",
        "for chunk in df:\n",
        "  i =0\n",
        "  chunk.groupby('Score')['Summary'].agg(word_search)\n",
        "  # break\n",
        "\n",
        "df_words =pd.DataFrame(words)\n",
        "#nested loops\n",
        "for data in df_words.columns:\n",
        "  p =sns.catplot(x= df_words.index,\n",
        "              y = df_words[data].values,\n",
        "              data =df_words,\n",
        "              kind ='bar'\n",
        "              )\n",
        "  for ind, val in enumerate(df_words[data].values):\n",
        "    p.ax.annotate(val,\n",
        "                  xy =(ind, val),\n",
        "                  xytext =(ind-0.20, val/2))\n",
        "  p.set(xlabel =\"Score\", ylabel = \"Count of Reviews\", title =data)\n",
        "  plt.show()\n",
        "# words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6y14dBB58ivR"
      },
      "source": [
        "#bad reviews\n",
        "df =read_file_chunks('new_file_Reviews.csv')\n",
        "STOPWORDS.extend(['good', 'great', 'like', 'love'])\n",
        "text = \" \".join(chunk[chunk['Sentiment'] ==-1].Summary.to_string() for chunk in df)\n",
        "wordcloud =WordCloud(stopwords=STOPWORDS).generate(text)\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QYozZSOb8vFg"
      },
      "source": [
        "#identifying if Summary or Text is null\n",
        "df =read_file_chunks('new_file_Reviews.csv')\n",
        "for chunk in df:\n",
        "  print(chunk[chunk.Summary.isna()].Score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wNZ-qZoaa5X8"
      },
      "source": [
        "#removing punctuations and getting the data that we will work on ready in file\n",
        "df =read_file_chunks(1)\n",
        "STOPWORDS = [word for word in STOPWORDS if word not in ['good', 'great', 'like', 'love']]\n",
        "def remove_punctuation(text):\n",
        "    # print(text)\n",
        "    final = \"\".join(u for u in str(text) if u not in (\"?\", \".\", \";\", \":\",  \"!\",'\"'))\n",
        "    return final\n",
        "\n",
        "#why use header...see new_file_Reviews creation in above cells\n",
        "header =True\n",
        "for chunk in df:\n",
        "  chunk['Text'] = chunk['Text'].apply(remove_punctuation)\n",
        "  chunk = chunk.dropna(subset=['Summary'])\n",
        "  chunk['Summary'] = chunk['Summary'].apply(remove_punctuation)\n",
        "  chunk[['Sentiment', 'Summary', 'Text']].to_csv(\"Summary_Text_Sentiment.csv\",index =False, header =header, mode='a')\n",
        "  header =False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MneLOZ2rfHnA"
      },
      "source": [
        "#checking if files have consistant records\n",
        "df =read_file_chunks(2)\n",
        "i=0\n",
        "for chunk in df:\n",
        "  i +=chunk.shape[0]\n",
        "\n",
        "print(i)\n",
        "print(len(STOPWORDS))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eVKn7zOdjPQI"
      },
      "source": [
        "df =read_file_chunks(2)\n",
        "next(df).head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZ2j0PAnmuxM"
      },
      "source": [
        "#Train & Test split\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "df =read_file_chunks(2)\n",
        "#X is data and Y is target\n",
        "header =True\n",
        "for chunk in df:\n",
        "  X_train, X_test, Y_train, Y_test =train_test_split(chunk['Summary'],\n",
        "                                                     chunk['Sentiment'],\n",
        "                                                     test_size =.2,\n",
        "                                                     random_state =21,\n",
        "                                                     stratify =chunk['Sentiment']\n",
        "                                                     )\n",
        "  X_train.to_csv(\"X_train.csv\",index =False, header =header, mode='a')\n",
        "  X_test.to_csv(\"X_test.csv\",index =False, header =header, mode='a')\n",
        "  Y_train.to_csv(\"Y_train.csv\",index =False, header =header, mode='a')\n",
        "  Y_test.to_csv(\"Y_test.csv\",index =False, header =header, mode='a')\n",
        "  header =False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9N8m0zaaq8St"
      },
      "source": [
        "#deleting unnecessary files\n",
        "import os\n",
        "os.remove(\"Summary_Text_Sentiment.csv\")\n",
        "os.remove(\"new_file_Reviews.csv\")\n",
        "os.remove(\"Reviews.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xTa0il9pxvx0"
      },
      "source": [
        "#access X_train, X_test, Y_train, Y_test in chunks\n",
        "def read_xy_chunk(file_name):\n",
        "  return pd.read_csv(file_name, chunksize =10000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fF-a7pzB7T5L"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "vectorizer = CountVectorizer(token_pattern=r'\\b\\w+\\b')\n",
        "train =pd.read_csv(\"X_train.csv\")\n",
        "test =pd.read_csv(\"X_test.csv\")\n",
        "train_matrix = vectorizer.fit_transform(train['Summary'].astype('str'))\n",
        "test_matrix = vectorizer.transform(test['Summary'].astype('str'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iwIVQXtWr_um"
      },
      "source": [
        "# count vectorizer:\n",
        "# from sklearn.feature_extraction.text import CountVectorizer\n",
        "# from sklearn.metrics import confusion_matrix,classification_report\n",
        "\n",
        "# vectorizer = CountVectorizer(token_pattern=r'\\b\\w+\\b')\n",
        "# def func(text):\n",
        "  # k =[word for word in text]\n",
        "  # text =\"\".join(str(text.values))\n",
        "  # final = \"\".join(u for u in str(text) if u not in (\"'\"))\n",
        "  # return final\n",
        "\n",
        "# Y_train = read_xy_chunk('Y_train.csv')\n",
        "# X_train = read_xy_chunk('X_train.csv')\n",
        "# for chunk, y in zip(X_train, Y_train):\n",
        "  # print(chunk.apply(func))\n",
        "  # break chunk.apply(func)\n",
        "  # train_matrix =vectorizer.fit_transform(chunk.Summary.values.astype('str'))\n",
        "  # print(vectorizer.get_feature_names())\n",
        "  # print(train_matrix.toarray())\n",
        "  # print(train_matrix.shape, y.shape)\n",
        "#   lr.fit(train_matrix, y['Sentiment'])\n",
        "\n",
        "# X_test = read_xy_chunk('X_test.csv')\n",
        "# Y_test = read_xy_chunk('Y_test.csv')\n",
        "\n",
        "# for chunk, y in zip(X_test, Y_test):\n",
        "#   test_matrix =vectorizer.fit_transform(chunk.Summary.values.astype('str'))\n",
        "#   predictions = lr.predict(chunk.Summary.values.astype('str'))\n",
        "#   new = np.asarray(y)\n",
        "#   print(confusion_matrix(predictions,y))\n",
        "# print(train_matrix)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uRoO0eCMzQ0X"
      },
      "source": [
        "# Logistic Regression\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "lr = LogisticRegression()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b8kNEYRW82pu"
      },
      "source": [
        "X_train = train_matrix\n",
        "X_test = test_matrix\n",
        "y_train = pd.read_csv(\"Y_train.csv\")['Sentiment']\n",
        "y_test = pd.read_csv(\"Y_test.csv\")['Sentiment']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Otpmtxz9Mvs"
      },
      "source": [
        "lr.fit(X_train,y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sKZdjf_h9Xp0"
      },
      "source": [
        "predictions = lr.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KGp5DuqS9ddq",
        "outputId": "315c754e-c6d4-4926-9e03-476aea7b0257"
      },
      "source": [
        "# find accuracy, precision, recall:\n",
        "from sklearn.metrics import confusion_matrix,classification_report\n",
        "new = np.asarray(y_test)\n",
        "confusion_matrix(predictions,y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[10631,  1907,  2045],\n",
              "       [  612,  1920,   949],\n",
              "       [ 5162,  4699, 85761]])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "48H8JLon9jtL",
        "outputId": "120a1e01-04c9-4abb-e5dd-9a6887ae1627"
      },
      "source": [
        "print(classification_report(predictions,y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1       0.65      0.73      0.69     14583\n",
            "           0       0.23      0.55      0.32      3481\n",
            "           1       0.97      0.90      0.93     95622\n",
            "\n",
            "    accuracy                           0.86    113686\n",
            "   macro avg       0.61      0.73      0.65    113686\n",
            "weighted avg       0.90      0.86      0.88    113686\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J9aVYftc9-lC",
        "outputId": "87efc289-6f5c-4eba-f0f8-543d82a93722"
      },
      "source": [
        "arr =vectorizer.transform(['These \"Butter Leaves\" are addictively wonderful!'])\n",
        "predictions = lr.predict(arr)\n",
        "print(predictions)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oYDX3sprzsbu"
      },
      "source": [
        "# !pip install django==1.11.17\n",
        "# !apt install python3.7\n",
        "# !python3.7 --version\n",
        "\n",
        "#fitting model\n",
        "X_train = train_matrix\n",
        "X_test = test_matrix\n",
        "Y_train = pd.read_csv('Y_train.csv')\n",
        "Y_test = read_xy_chunk('Y_test.csv')\n",
        "\n",
        "# for X_chunk, Y_chunk in zip(X_train, Y_train):\n",
        "#   print(X_chunk)\n",
        "# print(X_train.shape, Y_train.shape)\n",
        "lr.fit(X_train, Y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bva-G6Is0fLY"
      },
      "source": [
        "#making prediction \n",
        "predictions = lr.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}